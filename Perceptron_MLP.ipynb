{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejemplo simple con una sola red neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cargamos las 4 combinaciones de las compuertas XOR\n",
    "training_data = np.array([[0,0],[0,1],[1,1],[1,0],[0,0],[0,1],[1,1],[1,0],[0,0],[0,1],[1,1],[1,0],\n",
    "                          [0,0],[0,1],[1,1],[1,0],[0,0],[0,1],[1,1],[1,0],[0,0],[0,1],[1,1],[1,0]], \"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y estos son los resultados que se obtienen, en el mismo orden\n",
    "#target_data = np.array([[0],[3],[3],[3],[0],[3],[3],[3],[0],[3],[3],[3],\n",
    "#                        [0],[3],[3],[3],[0],[3],[3],[3],[0],[3],[3],[3]], \"float32\")\n",
    "target_data = np.array([[0],[1],[1],[1],[0],[1],[1],[1],[0],[1],[1],[1],\n",
    "                        [0],[1],[1],[1],[0],[1],[1],[1],[0],[1],[1],[1]], \"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joseg\\.conda\\envs\\datascience\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Perceptron(alpha=0.0001, class_weight=None, early_stopping=False, eta0=1.0,\n",
       "           fit_intercept=True, max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
       "           penalty=None, random_state=0, shuffle=True, tol=0.001,\n",
       "           validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "clf_Perceptron = Perceptron()\n",
    "clf_Perceptron.fit(training_data, target_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf_Perceptron.predict(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(target_data, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pesos =clf_Perceptron.coef_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pesos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pesos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 2.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pesos[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vamos con otro ejemplo para consolidar conocimientos\n",
    "https://www.interactivechaos.com/manual/tutorial-de-machine-learning/perceptron-multicapa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las arquitecturas vistas hasta ahora (redes neuronales formadas por capas de neuronas, navegando los datos desde la capa de entrada hasta la capa de salida, etc.) se corresponden con lo que se denomina perceptrón multicapa. A pesar del nombre, esta arquitectura no está basada en perceptrones, simplemente se ha conservado el término \"percentrón\" en el nombre por motivos históricos.\n",
    "\n",
    "Scikit-Learn implementa el perceptrón multicapa para clasificación y regresión en las clases sklearn.neural_network.MLPClassifier y sklearn.neural_network.MLPRegressor respectivamente.\n",
    "\n",
    "Veamos un sencillo ejemplo. Supongamos que partimos del siguiente dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0</th>\n",
       "      <th>x1</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    x0   x1  y\n",
       "0  0.3  0.7  1\n",
       "1  0.1  0.2  0\n",
       "2  0.4  0.3  0\n",
       "3  0.5  0.8  1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.DataFrame({\n",
    "    \"x0\": [0.3, 0.1, 0.4, 0.5],\n",
    "    \"x1\": [0.7, 0.2, 0.3, 0.8],\n",
    "    \"y\": [1, 0, 0, 1]\n",
    "})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En x0 y x1 tenemos las características predictivas y en y la variable objetivo, que vamos a considerar categórica para trabajar en un escenario de clasificación. Como es habitual, creamos los conjuntos con las características predictivas y la variable objetivo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df.pop(\"y\")\n",
    "X_train = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objetivo es entrenar una red neuronal con una capa oculta de 2 neuronas. En Scikit-Learn las clases MLPClassifier y MLPRegressor reciben como primer argumento el parámetro hidden_layer_sizes, que controla la arquitectura de la red neuronal. Este argumento debe ser una tupla cuyos valores representen el número de neuronas artificiales en cada capa oculta. Así, (2, ) representaría una única capa oculta con 2 neuronas (la coma es necesaria para evitar confundir el \"2\" con un número entero), o (5, 3) representaría dos capas ocultas con 5 y 3 neuronas artificiales respectivamente. En todo caso, la implementación de Scikit-Learn incluye un par de detalles que debemos conocer:\n",
    "\n",
    "La capa de entrada está formada por los valores que toman las características con las que se va a alimentar la red. Es decir, en esta capa de entrada no se va a aplicar ninguna función a los datos.\n",
    "En regresión, la capa de salida está formada por una única neurona.\n",
    "En clasificación, la capa de salida está formada por tantas neuronas como clases, excepto si se trata de clasificación binaria, en cuyo caso solo hay una neurona cuya función de activación determina la clase a devolver como predicción.\n",
    "Es decir, la arquitectura de la red neuronal que vamos a crear es la siguiente:\n",
    "[Imgur](https://imgur.com/yWnJm1X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..donde:\n",
    "\n",
    "x0 y x1 son la características homónimas para cada muestra que se haga pasar por la red\n",
    "N0 y N1 son las dos neuronas que componen la capa oculta\n",
    "S es la neurona que compone la capa de salida (una sola pues estamos trabajando en un entorno de clasificación binaria)\n",
    "w00 es el peso por el que se va a multiplicar x0 en la entrada a la neurona N0\n",
    "w01 es el peso por el que se va a multiplicar x0 en la entrada a la neurona N1\n",
    "w10 es el peso por el que se va a multiplicar x1 en la entrada a la neurona N0\n",
    "w11 es el peso por el que se va a multiplicar x1 en la entrada a la neurona N1\n",
    "b0 es el bias a sumar a los valores de entrada ponderados de la neurona N0\n",
    "b1 es el bias a sumar a los valores de entrada ponderados de la neurona N1\n",
    "w20 es el peso por el que se va a multiplicar la salida de la neurona N0 en la entrada a la neurona de salida S\n",
    "w21 es el peso por el que se va a multiplicar la salida de la neurona N1 en la entrada a la neurona de salida S\n",
    "b2 es el bias a sumar a los valores de entrada ponderados de la neurona de salida N\n",
    "En resumen, trabajaremos con 6 pesos (llamados coeficientes en Scikit-Learn) y tres valores (bias) a añadir a los valores de entrada ponderados de las neuronas (llamados interceptores en Scikit-Learn).\n",
    "\n",
    "Importamos ahora la clase MLPClassifier, instanciamos el modelo y lo entrenamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "model = MLPClassifier((2, ), random_state = 0, learning_rate_init = 0.1, activation = \"logistic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estamos creando el modelo con una tasa de aprendizaje de 0.1 (con tan pocas muestras en el conjunto de entrenamiento haría falta pasar los datos muchas veces por la red -muchas \"epochs\"- para conseguir que el algoritmo convergiese si utilizásemos la tasa de aprendizaje por defecto de 0.001). Especificamos también que como función de activación en la capa oculta se use la función logística."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='logistic', alpha=0.0001, batch_size='auto',\n",
       "              beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(2,), learning_rate='constant',\n",
       "              learning_rate_init=0.1, max_fun=15000, max_iter=200, momentum=0.9,\n",
       "              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "              random_state=0, shuffle=True, solver='adam', tol=0.0001,\n",
       "              validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='logistic', alpha=0.0001, batch_size='auto',\n",
       "              beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(2,), learning_rate='constant',\n",
       "              learning_rate_init=0.1, max_fun=15000, max_iter=200, momentum=0.9,\n",
       "              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "              random_state=0, shuffle=True, solver='adam', tol=0.0001,\n",
       "              validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Del resumen devuelto por el modelo al ser entrenado vemos que, por defecto, se realiza un máximo de 200 iteraciones (\"epochs\") y el \"solver\" utilizado es adam.\n",
    "\n",
    "Veamos cuál es la predicción del modelo para los mismos valores con los que se ha entrenado y su precisión media:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo generado tiene dos atributos que nos dan acceso a los coeficientes y a los interceptores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.14736599, -1.15400831],\n",
       "        [-7.83405221,  9.72328823]]),\n",
       " array([[-6.60705046],\n",
       "        [ 6.00857268]])]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.coefs_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 3.89827002, -4.41150649]), array([0.20379602])]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.intercepts_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El atributo coefs_ devuelve un array con tantos elementos como capas (excluyendo la de entrada que ya se ha comentado que no va a aplicar ninguna transformación), estando formado cada elemento por una matriz de coeficientes (de pesos). El atributo intercepts_ devuelve otro array en el que cada elemento contiene el vector de bias de la capa correspondiente.\n",
    "\n",
    "Ahora podemos llevar estos valores a nuestro esquema con la arquitectura de la red neuronal que acabamos de crear:\n",
    "<blockquote class=\"imgur-embed-pub\" lang=\"en\" data-id=\"h9gYbSn\"><a href=\"https://imgur.com/h9gYbSn\">View post on imgur.com</a></blockquote><script async src=\"//s.imgur.com/min/embed.js\" charset=\"utf-8\"></script>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(se muestran en azul los coeficientes y en verde los bias).\n",
    "\n",
    "Una vez entrenada la red, para realizar una predicción, por ejemplo para el valor (x0 = 0.8, x1 = 0.3), bastaría con hacer pasar dicho valor por la red, aplicándose las funciones vistas en cada nodo, hasta alcanzar la capa de salida. La neurona artificial de esta capa de salida (solo una en el caso de clasificación binaria) aplica una función sigmoidea (con independencia de la función de activación usada en el resto de la red) al dato que resulta de ponderar y sumar sus datos de entrada, convirtiendo dicho dato en una probabilidad y deduciendo, a partir de ella, la clase a asignar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pesos=model.coefs_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.14736599 -1.15400831]\n",
      " [-7.83405221  9.72328823]]\n"
     ]
    }
   ],
   "source": [
    "print(pesos[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.14736599 -7.83405221]\n",
      " [-1.15400831  9.72328823]]\n"
     ]
    }
   ],
   "source": [
    "N=np.transpose(pesos[0])\n",
    "print(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-6.60705046],\n",
       "       [ 6.00857268]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pesos[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-6.60705046  6.00857268]]\n"
     ]
    }
   ],
   "source": [
    "S=np.transpose(pesos[1])\n",
    "print(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
